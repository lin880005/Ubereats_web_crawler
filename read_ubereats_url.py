from selenium import webdriver # ç€è¦½å™¨
from selenium.webdriver.common.by import By # é¸å–å™¨
from selenium.webdriver.support.wait import WebDriverWait # ç¶²ç«™ç­‰å¾…
from selenium.webdriver.support import expected_conditions as EC # å…ƒç´ ç‹€æ…‹åˆ¤æ–·
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import json
import openpyxl
import re
import pandas as pd

def data_clean(text):
    # æ¸…æ´—excelä¸­çš„éæ³•å­—ç¬¦ï¼Œéƒ½æ˜¯ä¸å¸¸è§çš„ä¸å¯æ˜¾ç¤ºå­—ç¬¦ï¼Œä¾‹å¦‚é€€æ ¼ï¼Œå“é“ƒç­‰
    ILLEGAL_CHARACTERS_RE = re.compile(r"[\000-\010]|[\013-\014]|[\016-\037]")
    text = ILLEGAL_CHARACTERS_RE.sub(r"", text)
    return text

options = Options()
options.add_argument("--disable-notifications") # é—œé–‰å½ˆå‡ºå¼è¦–çª—
driver = webdriver.Chrome(chrome_options = options)

driver.get("https://www.ubereats.com/tw")

getblock = driver.find_element(By.XPATH, '//*[@placeholder="è¼¸å…¥å¤–é€åœ°å€"]')
getblock.send_keys('é«˜é›„å¸‚')
time.sleep(1)
getblock.send_keys('\ue007') # æŒ‰ä¸‹Enter
time.sleep(3)


wb = openpyxl.Workbook()
ws = wb.active



ws["A1"] = "é¤å»³åç¨±"
ws["B1"] = "é¤å»³é¡å‹"
ws["C1"] = "é¤å»³ç¸½è©•åˆ†"
ws["D1"] = "åœ°å€"
ws["E1"] = "ç¶“åº¦"
ws["F1"] = "ç·¯åº¦"
ws["G1"] = "è¨‚é¤ç¶²å€"


df = pd.read_excel('Uber_eatsé«˜é›„å¸‚é¤å»³ç¶²å€.xlsx')

# æå–ç¶²å€æ¬„ä½çš„æ•¸æ“š
urls = df['URL']

count = 0

for store in urls:
    driver.get(store)
    time.sleep(3)
    try:         # ğŸ‘ˆğŸ‘€ æ­£å¸¸æ¥å–®çš„åº—å®¶ï¼Œå¯ç›´æ¥é»é¸åˆ° "è©³ç´°è³‡è¨Š"ï¼Œèµ°try ğŸ“Œ

        detail = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[text()="è©³ç´°è³‡è¨Š"]')))
        detail.click()
        time.sleep(5)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        #print(soup)

        info = soup.find_all("main", id="main-content")[0].script.text
        dic_info = json.loads(info)
        #print(info)
        name = dic_info["name"]  # åº—å
        
        try:
            sc = dic_info["aggregateRating"]["ratingValue"]  # ç¸½è©•åˆ†
            ad = dic_info["address"]["streetAddress"]                       # ğŸ‘ˆğŸ‘€ å…ˆä¸è¦æŠ“åœ°å€ ğŸ“Œ
            type = dic_info["servesCuisine"][0]  # é¡å‹
            lo = dic_info["geo"]["longitude"]#ç¶“åº¦
            la = dic_info["geo"]["latitude"]#ç·¯åº¦
        except:
            sc = "NoRating"
            ad = ""
            type = ""
            lo = "" #ç¶“åº¦
            la = "" #ç·¯åº¦
        count+=1
        print(f"====ç¬¬{count}é–“====")

    except:       # ğŸ‘ˆğŸ‘€ å·²ä¸æ¥å–®çš„åº—å®¶ï¼Œæœ‰å½ˆå‡ºè¦–çª—å°è‡´ç„¡æ³•ç›´æ¥é»é¸åˆ° "è©³ç´°è³‡è¨Š"ï¼Œèµ°except ğŸ“Œ

        closeButton = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[aria-label="é—œé–‰"]')))
        closeButton.click()
        
        # â˜ğŸ‘€ å¤šä¸€å€‹æ­¥é©Ÿï¼Œè¦å…ˆæŠŠå½ˆå‡ºçš„è¦–çª—é—œé–‰ ğŸ“Œ
        # ğŸ‘‡ğŸ‘€ å¾Œé¢ä¸€å¤§å¨å°±è·Ÿå‰é¢çš„tryä¸€æ¨£ï¼Œé»é¸ "è©³ç´°è³‡è¨Š" æŠ“åº—å®¶è³‡æ–™ ğŸ“Œ
        
        detail = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[text()="è©³ç´°è³‡è¨Š"]')))
        detail.click()
        time.sleep(5)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        #print(soup)

        info = soup.find_all("main", id="main-content")[0].script.text
        dic_info = json.loads(info)
        #print(info)
        name = dic_info["name"]  # åº—å
        
        try:
            sc = dic_info["aggregateRating"]["ratingValue"]  # ç¸½è©•åˆ†
            ad = dic_info["address"]["streetAddress"]    # ğŸ‘ˆğŸ‘€ å…ˆä¸è¦æŠ“åœ°å€ ğŸ“Œ
            type = dic_info["servesCuisine"][0]  # é¡å‹
            lo = dic_info["geo"]["longitude"]#ç¶“åº¦
            la = dic_info["geo"]["latitude"]#ç·¯åº¦                   
        except:
            sc = "NoRating"
            ad = ""
            type = ""
            lo = "" #ç¶“åº¦
            la = "" #ç·¯åº¦
        count+=1
        print(f"====ç¬¬{count}é–“====")
 

    ws.append([data_clean(name),type,sc,ad,lo,la, store])
    wb.save("Uber_eatsé«˜é›„å¸‚.xlsx")
driver.quit()